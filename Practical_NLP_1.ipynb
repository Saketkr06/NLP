{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwcGf1aSoARzgkcIDKb0Nt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saketkr06/NLP/blob/main/Practical_NLP_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "MZrTSXaalJth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40256ef7-11fd-4bd9-94b3-8774c2811c84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "documents=[\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "processed_docs=[doc.lower().replace('.','') for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={}\n",
        "count=0\n",
        "for doc in processed_docs:\n",
        "  for word in doc.split():\n",
        "    if word not in vocab:\n",
        "      count+=1\n",
        "      vocab[word]=count\n",
        "\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDAKvbxK4yse",
        "outputId": "8af735a4-ae18-453d-a404-1311414387e7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_onehot_vector(somestring):\n",
        "  onehot_encoded=[]\n",
        "  for word in somestring.split():\n",
        "    temp=[0]*len(vocab)\n",
        "    if word in vocab:\n",
        "      temp[vocab[word]-1]=1\n",
        "\n",
        "    onehot_encoded.append(temp)\n",
        "\n",
        "  return onehot_encoded"
      ],
      "metadata": {
        "id": "apVuGaiq5QKz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_docs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XI_NFy7J6Kjs",
        "outputId": "14e909e9-d7d0-493a-9037-c40cd0984a62"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'man bites dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_onehot_vector(processed_docs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4zHitZc6Ogl",
        "outputId": "3f8cfc7d-698f-4c74-b47f-1aea42f7e262"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S1 = 'dog bites man'\n",
        "S2 = 'man bites dog'\n",
        "S3 = 'dog eats meat'\n",
        "S4 = 'man eats food'"
      ],
      "metadata": {
        "id": "hkpMNXas67qs"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
        "values = data[0]+data[1]+data[2]+data[3]\n",
        "print(\"The data: \",values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX_FDQmS6Trq",
        "outputId": "7c4938c0-fd30-4ae9-b5e6-3874062b569c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data:  ['dog', 'bites', 'man', 'man', 'bites', 'dog', 'dog', 'eats', 'meat', 'man', 'eats', 'food']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder=LabelEncoder()\n",
        "integer_encoded=label_encoder.fit_transform(values)\n",
        "integer_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll63uIQZ64K1",
        "outputId": "cc777ab5-d548-4bf4-ef2d-56aab312ca2e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 4, 4, 0, 1, 1, 2, 5, 4, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One-Hot Encoding\n",
        "onehot_encoder = OneHotEncoder()\n",
        "onehot_encoded = onehot_encoder.fit_transform(data).toarray()\n",
        "print(\"Onehot Encoded Matrix:\\n\",onehot_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHD-ws6_7NlG",
        "outputId": "89f85a0e-800e-44c3-cc14-82d2ecc9ee0f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Onehot Encoded Matrix:\n",
            " [[1. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words"
      ],
      "metadata": {
        "id": "CxjBMW287ooX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect=CountVectorizer()\n",
        "bow_rep=count_vect.fit_transform(processed_docs)\n",
        "\n",
        "print('Our Vocabulary',count_vect.vocabulary_)\n",
        "print( \"BoW representation for 'dog bites man': \",bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
        "\n",
        "#Get the representation using this vocabulary, for a new text\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y26hntBD7QJL",
        "outputId": "5c4f20dc-8bec-43dc-d8ee-03c5f856f9c4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Vocabulary {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
            "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
            "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n",
            "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BoW with binary vectors\n",
        "count_vect = CountVectorizer(binary=True)\n",
        "count_vect.fit(processed_docs)\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MdMkQyI8yQW",
        "outputId": "296df367-0e81-43d2-8761-455c219a74ee"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUaUpSqR86ze"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BAG of N-Grams"
      ],
      "metadata": {
        "id": "co7FWEP29B-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect=CountVectorizer(ngram_range=(1,3))\n",
        "#Build a BOW representation for the corpus\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "#Look at the vocabulary mapping\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "#see the BOW rep for first 2 documents\n",
        "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
        "\n",
        "#Get the representation using this vocabulary, for a new text\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pjjD5VB9Er-",
        "outputId": "b666ae87-9f6d-45c8-e9a8-4ab2d7ea1461"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
            "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
            "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
            "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "bow_rep_tfidf=tfidf.fit_transform(processed_docs)\n",
        "#IDF for all words in the vocabulary\n",
        "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
        "print(\"-\"*10)\n",
        "#All words in the vocabulary.\n",
        "print(\"All words in the vocabulary\",tfidf.get_feature_names_out)\n",
        "print(\"-\"*10)\n",
        "\n",
        "#TFIDF representation for all documents in our corpus\n",
        "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray())\n",
        "print(\"-\"*10)\n",
        "\n",
        "temp = tfidf.transform([\"dog and man are friends\"])\n",
        "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFUJ6dL59Zrj",
        "outputId": "d4f2b92d-1a07-4546-d12b-db916428f4a4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
            "----------\n",
            "All words in the vocabulary <bound method CountVectorizer.get_feature_names_out of TfidfVectorizer()>\n",
            "----------\n",
            "TFIDF representation for all documents in our corpus\n",
            " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
            " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
            "----------\n",
            "Tfidf representation for 'dog and man are friends':\n",
            " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Replace 'file_id' with the actual file ID from your Google Drive link\n",
        "file_id = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
        "\n",
        "# URL to download the file\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Path to save the downloaded file\n",
        "downloaded_file_path = '/content/word2vec.bin.gz'\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "with open(downloaded_file_path, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"File downloaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJYJ7DaG-P56",
        "outputId": "f79be4c8-7e8c-4cca-9b24-2dac1ae21df5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# installing the Kaggle library\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhkX28fA_PUo",
        "outputId": "0a84731f-4727-49d5-9feb-d671ef473563"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.16)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configuring the path of Kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "9wuu_Laq_x5X"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d leadbest/googlenewsvectorsnegative300"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN9KG4STDHp0",
        "outputId": "3aa19fac-ac13-4994-f744-3d17cfcd23cf"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "googlenewsvectorsnegative300.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the compressed dataset\n",
        "from zipfile import ZipFile\n",
        "\n",
        "dataset = '/content/googlenewsvectorsnegative300.zip'\n",
        "\n",
        "with ZipFile(dataset, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('The dataset is extracted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv84Gqt3DMhW",
        "outputId": "5d2001a5-58a2-454b-9c68-388f8aa5b518"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset is extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings #This module ignores the various types of warnings generated\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
        "process = psutil.Process(os.getpid())\n",
        "from psutil import virtual_memory\n",
        "mem = virtual_memory()\n",
        "\n",
        "import time #This module is used to calculate the time"
      ],
      "metadata": {
        "id": "gTyb-cuIDa-n"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "gen_vec_path='/content/GoogleNews-vectors-negative300.bin'\n",
        "pretrainedpath=gen_vec_path\n",
        "pre=process.memory_info().rss\n",
        "\n",
        "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
        "print('-'*10)\n",
        "\n",
        "start_time = time.time() #Start the timer\n",
        "ttl = mem.total #Toal memory available\n",
        "\n",
        "w2v_model=KeyedVectors.load_word2vec_format(pretrainedpath,binary=True)\n",
        "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
        "print('-'*10)\n",
        "\n",
        "print('Finished loading Word2Vec')\n",
        "print('-'*10)\n",
        "\n",
        "post = process.memory_info().rss\n",
        "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
        "print('-'*10)\n",
        "\n",
        "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
        "print('-'*10)\n",
        "\n",
        " #Number of words in the vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM4CxpmcDvb-",
        "outputId": "70a9c6c0-da84-41be-8c38-bcc10e0c7558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory used in GB before Loading the Model: 9.07\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Numver of words in vocablulary: \",len(w2v_model.key_to_index))"
      ],
      "metadata": {
        "id": "1hLr5yaAG440"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.most_similar('beautiful')"
      ],
      "metadata": {
        "id": "Pc9pikC0EuiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us try with another word!\n",
        "w2v_model.most_similar('toronto')"
      ],
      "metadata": {
        "id": "7w5i4GhpH5R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the vector representation for a word?\n",
        "w2v_model['computer']"
      ],
      "metadata": {
        "id": "hKxSlNxGICIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "dvwW4BLAIFTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_md')\n",
        "mydoc=nlp('India is most populated country')\n",
        "print(mydoc.vector)"
      ],
      "metadata": {
        "id": "22RFBhK4IWKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "yjUJ5J9xI0gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training data\n",
        "#Genism word2vec requires that a format of ‘list of lists’ be provided for training where every document contained in a list.\n",
        "#Every list contains lists of tokens of that document.\n",
        "corpus = [['dog','bites','man'], [\"man\", \"bites\" ,\"dog\"],[\"dog\",\"eats\",\"meat\"],[\"man\", \"eats\",\"food\"]]"
      ],
      "metadata": {
        "id": "tu4q0tTgJWIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cbow=Word2Vec(corpus,min_count=1,sg=0)\n",
        "model_skipgram=Word2Vec(corpus,min_count=1,sg=1)"
      ],
      "metadata": {
        "id": "PktWGW7rJZhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summarize the loaded model\n",
        "print(model_cbow)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(model_cbow.wv.index_to_key)\n",
        "print(words)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(model_cbow.wv['dog'])"
      ],
      "metadata": {
        "id": "9JlXiomMJ2ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute similarity\n",
        "print(\"Similarity between eats and bites:\",model_cbow.similarity('eats', 'bites'))\n",
        "print(\"Similarity between eats and man:\",model_cbow.similarity('eats', 'man'))"
      ],
      "metadata": {
        "id": "X4LgFoRwJ7sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Most similarity\n",
        "model_cbow.wv.most_similar('meat')"
      ],
      "metadata": {
        "id": "JPB8Q34RM8BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model_cbow.save('model_cbow.bin')\n",
        "\n",
        "# load model\n",
        "new_model_cbow = Word2Vec.load('model_cbow.bin')\n",
        "print(new_model_cbow)"
      ],
      "metadata": {
        "id": "okwbOTZ5OGke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summarize the loaded model\n",
        "print(model_skipgram)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(model_skipgram.wv.index_to_key)\n",
        "print(words)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(model_skipgram.wv['dog'])"
      ],
      "metadata": {
        "id": "PoEtxxlvOJWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "os.makedirs('data/en', exist_ok= True)\n",
        "file_name = \"data/en/enwiki-latest-pages-articles-multistream14.xml-p13159683p14324602.bz2\"\n",
        "file_id = \"11804g0GcWnBIVDahjo5fQyc05nQLXGwF\"\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    download_file_from_google_drive(file_id, file_name)\n",
        "else:\n",
        "    print(\"file already exists, skipping download\")\n",
        "\n",
        "print(f\"File at: {file_name}\")"
      ],
      "metadata": {
        "id": "dH_3nnu-ORPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.wikicorpus import WikiCorpus\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.fasttext import FastText\n",
        "import time"
      ],
      "metadata": {
        "id": "1F_XOYJqOgpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki=WikiCorpus(file_name,dictionary={})\n",
        "sentences=list(wiki.get_texts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPIu031EOk0o",
        "outputId": "569e1225-5b2d-4d2d-9b4e-fa6b26c6c03e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process InputQueue-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gensim/utils.py\", line 1292, in run\n",
            "    wrapped_chunk = [list(chunk)]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gensim/corpora/wikicorpus.py\", line 682, in <genexpr>\n",
            "    texts = (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gensim/corpora/wikicorpus.py\", line 412, in extract_pages\n",
            "    elem = next(elems)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gensim/corpora/wikicorpus.py\", line 406, in <genexpr>\n",
            "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
            "  File \"/usr/lib/python3.10/xml/etree/ElementTree.py\", line 1255, in iterator\n",
            "    data = source.read(16 * 1024)\n",
            "  File \"/usr/lib/python3.10/bz2.py\", line 164, in read\n",
            "    return self._buffer.read(size)\n",
            "  File \"/usr/lib/python3.10/_compression.py\", line 68, in readinto\n",
            "    data = self.read(len(byte_view))\n",
            "  File \"/usr/lib/python3.10/_compression.py\", line 103, in read\n",
            "    data = self._decompressor.decompress(rawblock, size)\n",
            "OSError: Invalid data stream\n",
            "WARNING:gensim.corpora.wikicorpus:user terminated iteration over Wikipedia corpus after 0 documents with 0 positions (total 0 articles, 0 positions before pruning articles shorter than 50 words)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RfbfVuzFPv-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}